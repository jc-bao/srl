# wandb
wandb: false
entity: jc-bao
# set load path to load locally
load_path: null
load_curri: null
load_tag: latest
load_project: null
load_folder: ''
# set wandb wid to load remotely
wid: null
# resume mode {null, 'continue', 'restart', 'load_only'}
resume_mode: null 
project: debug 
name: debug

# seed 
random_seed: 0

# replay
her_rate: 0.8
# random relabel umoved objects' goal
g_random_relabel_rate: 0.8
g_random_relabel_bar: 5
# random relabel umoved objects' pos
ag_random_relabel_rate: 0.0
# prioterity replay
if_use_per: false
buffer_size: 4000000
# if store the early terminated experience (recommend: false)
dropout_early_termin: false

# env_params
env_name: 'FrankaPNP-v0'
# env_params to use in agent, set to false to load automatically (recommend: false)
env_params: null
  
reward_scale: 1

# network
act_net: Actor
actor_pool_type: max
critic_pool_type: mean
critic_net: CriticTwin
net_dim: 256
net_layer: 4
# attention block number
shared_net_layer: 3
n_head: 4 # attention params
net_type: attn # {'mlp', 'dense', 'attn'}
if_act_target: true
if_cri_target: true
gpu_id: 0

# algorithm
agent_name: AgentTD3
max_collect_steps: !!float 1e9
reuse: 100
# steps collect between updates, set to null to calculate automatically(minimum steps * env number)
steps_per_rollout: null # recommend: null
# network repeat update times between updates, set to null to calculate automatically(use reuse)
updates_per_rollout: null
batch_size: 4096
gamma: 0.98
lr: 0.00048
soft_update_tau: 0.005
explore_noise: 0.2
# TD3
update_freq: 2
policy_noise: 0.3
policy_update_gap: 1 # after $ of Q updates, update policy

# eval
eval_per_rollout: 0.25
eval_eps: null # set automatically with steps per rollout
# if upload render to wandb
render: false
render_per_eval: 0.2


defaults:
  - curri: gap_os_ho
  - env: 1ho

# save
cwd: results
eval_per_save: 0.1