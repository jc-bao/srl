{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffer store function (multi done scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import AgentSAC\n",
    "from attrdict import AttrDict\n",
    "import torch\n",
    "import yaml\n",
    "from envs import toy\n",
    "\n",
    "with open(f'configs/debug.yaml', \"r\") as stream:\n",
    "  try:\n",
    "    cfg = AttrDict(yaml.safe_load(stream))\n",
    "  except yaml.YAMLError as exc:\n",
    "    print(exc)\n",
    "agent = AgentSAC(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = agent.explore_vec_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.buffer.data_parser(agent.buffer.data[8:12], 'info.ag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = agent.buffer.sample_batch(10, her_rate=1, indices=torch.arange(11, device='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = agent.EP.obs_parser(trans.state[:10])\n",
    "info_dict = agent.EP.info_parser(trans.info[:10])\n",
    "obs_dict.ag, obs_dict.g, info_dict.step, trans.rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = agent.EP.info_parser(trans.info[:20])\n",
    "info_dict.step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HER buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Check Block Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_envs=2\n",
    "num_goals=3\n",
    "min_dist = 1\n",
    "extra_goals = torch.tensor(\n",
    "  [\n",
    "    [\n",
    "      [[0,0],[0.5,0],[1.5,0]],\n",
    "      [[0,0],[0.5,0.5],[1.5,0.5]]\n",
    "    ],\n",
    "    [\n",
    "      [[0,0],[1.5,0],[0,1.5]],\n",
    "      [[3,0],[0,0],[5,0]]\n",
    "    ],\n",
    "  ]\n",
    ")\n",
    "goal_dist = torch.abs(extra_goals.unsqueeze(-3) - extra_goals.unsqueeze(-2))\n",
    "satisfied_idx = ((goal_dist[...,0] > min_dist) | \\\n",
    "  (goal_dist[..., 1] > min_dist)) | torch.eye(num_goals).bool()\n",
    "satisfied_idx = satisfied_idx.all(dim=-1).all(dim=-1)\n",
    "extra_goals[satisfied_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.2\n",
    "n=3\n",
    "import math\n",
    "import numpy as np\n",
    "np.cumsum([math.factorial(n)/(math.factorial(n-m)*math.factorial(m))*(1-p)**m*p**(n-m) for m in range(n+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "grip_pos = torch.tensor([-1,-2,1,-2,2,1])\n",
    "hand_vel = torch.tensor([0.1,0.1,-0.1,-0.1,-0.1,0.1])\n",
    "finger_width = torch.tensor([0.01,0.02])\n",
    "block_state = torch.tensor([0]*20)\n",
    "block_pos = torch.tensor([-10,-20,10,10,20,-10])\n",
    "goal = torch.tensor([1,2,3,4,5,6])\n",
    "\n",
    "# obs = torch.cat((grip_pos, hand_vel, finger_width, block_state, block_pos, goal))\n",
    "\n",
    "# pos_rot_mat = torch.tensor([\n",
    "#   [-1,0,0],\n",
    "#   [0,-1,0],\n",
    "#   [0,0,1]\n",
    "# ])\n",
    "# quat_rot_mat = torch.tensor([\n",
    "#   [0,-1,0,0],\n",
    "#   [1,0,0,0],\n",
    "#   [0,0,0,1],\n",
    "#   [0,0,1,0],\n",
    "# ])\n",
    "# block_other_mat = torch.block_diag(*([quat_rot_mat]+[pos_rot_mat]*2))\n",
    "# rot_mat = torch.block_diag(*([pos_rot_mat]*4+[torch.eye(4)]+[block_other_mat]*2+[pos_rot_mat]*4))\n",
    "# print((obs*rot_mat)/obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_obs = torch.cat((grip_pos, hand_vel, finger_width))\n",
    "robot_reshape_mat = torch.tensor(\n",
    "  [\n",
    "    [1.,0,0]+[0]*11,\n",
    "    [0,1,0]+[0]*11,\n",
    "    [0,0,1]+[0]*11,\n",
    "    [0]*7+[1,0,0]+[0]*4,\n",
    "    [0]*7+[0,1,0]+[0]*4,\n",
    "    [0]*7+[0,0,1]+[0]*4,\n",
    "    [0]*3+[1,0,0]+[0]*8,\n",
    "    [0]*3+[0,1,0]+[0]*8,\n",
    "    [0]*3+[0,0,1]+[0]*8,\n",
    "    [0]*10+[1,0,0]+[0],\n",
    "    [0]*10+[0,1,0]+[0],\n",
    "    [0]*10+[0,0,1]+[0],\n",
    "    [0]*6+[1]+[0]*7,\n",
    "    [0]*13+[1],\n",
    "  ],\n",
    ")\n",
    "robot_obs@robot_reshape_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "num_goals = 3\n",
    "batch_size = 2\n",
    "num_heads = 1\n",
    "self_attn = nn.MultiheadAttention(4, num_heads)\n",
    "ten_in = torch.randn(num_goals,batch_size,4)\n",
    "mask = torch.tensor([[1,1,0], [1,0,0]]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token, weight = self_attn(ten_in[[0]], ten_in, ten_in, ~mask)\n",
    "token, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token[mask.transpose(0, 1) == 0] = 0.0\n",
    "token.sum(dim=0)/mask.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run = api.run(\"jc-bao/corl-2022/l3r9yo7c\") #   3q5t0kdk 1ml3z0m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_2 = api.run(\"jc-bao/corl-main/27th4unw\") #   25dqffwv klpwf6u6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"debug\", name=\"main-full\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i, row in old_run.history().iterrows():\n",
    "  row = row.to_dict()\n",
    "  data = {}\n",
    "  for k, v in row.items():\n",
    "    if not math.isnan(v):\n",
    "      data[k] = v\n",
    "  if 'eval/final_rew' in data and 'curri/current_num_goals' in data:\n",
    "    rew = data['eval/final_rew']\n",
    "    num_goals = data['curri/current_num_goals']\n",
    "    data['plot/arranged_block_number'] = num_goals * (rew+1)\n",
    "  for key, value in row.items():\n",
    "    wandb.log(data, step=int(row['_step']))\n",
    "  if row['_step'] > 3e7:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_step = int(row['_step']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i, row in old_run_2.history().iterrows():\n",
    "  row = row.to_dict()\n",
    "  data = {}\n",
    "  for k, v in row.items():\n",
    "    if not math.isnan(v):\n",
    "      data[k] = v\n",
    "  if 'eval/final_rew' in data and 'curri/current_num_goals' in data:\n",
    "    rew = data['eval/final_rew']\n",
    "    num_goals = data['curri/current_num_goals']\n",
    "    data['plot/arranged_block_number'] = num_goals * (rew+1)\n",
    "  for key, value in row.items():\n",
    "    wandb.log(data, step=int(row['_step'])+last_step)\n",
    "  # if row['_step'] > 4.32e7:\n",
    "  #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/pcy/rl/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/pcy/rl/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n\nforward(__torch__.net.AttnEncoderLayer self, Tensor src, Tensor mask) -> ((Tensor, Tensor)):\nArgument mask not provided.\n:\n  File \"/home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pcy/rl/srl/test.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22537061726b227d/home/pcy/rl/srl/test.ipynb#ch0000036vscode-remote?line=9'>10</a>\u001b[0m act \u001b[39m=\u001b[39m ActorFixSAC(cfg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22537061726b227d/home/pcy/rl/srl/test.ipynb#ch0000036vscode-remote?line=10'>11</a>\u001b[0m act\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mactor\u001b[39m\u001b[39m\"\u001b[39m], strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22537061726b227d/home/pcy/rl/srl/test.ipynb#ch0000036vscode-remote?line=11'>12</a>\u001b[0m scripted_policy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(act)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22537061726b227d/home/pcy/rl/srl/test.ipynb#ch0000036vscode-remote?line=12'>13</a>\u001b[0m scripted_policy\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mscripted.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py:1265\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1262'>1263</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1263'>1264</a>\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1264'>1265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1265'>1266</a>\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1266'>1267</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1268'>1269</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=1269'>1270</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:454\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=451'>452</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=452'>453</a>\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=453'>454</a>\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=512'>513</a>\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=514'>515</a>\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=515'>516</a>\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=517'>518</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=580'>581</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=581'>582</a>\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=582'>583</a>\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=590'>591</a>\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=591'>592</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=592'>593</a>\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=593'>594</a>\u001b[0m init_fn(script_module)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=595'>596</a>\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=596'>597</a>\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=597'>598</a>\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=490'>491</a>\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=491'>492</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=492'>493</a>\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=493'>494</a>\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=495'>496</a>\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=496'>497</a>\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=512'>513</a>\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=514'>515</a>\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=515'>516</a>\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=517'>518</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=580'>581</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=581'>582</a>\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=582'>583</a>\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=590'>591</a>\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=591'>592</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=592'>593</a>\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=593'>594</a>\u001b[0m init_fn(script_module)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=595'>596</a>\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=596'>597</a>\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=597'>598</a>\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=490'>491</a>\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=491'>492</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=492'>493</a>\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=493'>494</a>\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=495'>496</a>\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=496'>497</a>\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=512'>513</a>\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=514'>515</a>\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=515'>516</a>\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=517'>518</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=580'>581</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=581'>582</a>\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=582'>583</a>\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=590'>591</a>\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=591'>592</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=592'>593</a>\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=593'>594</a>\u001b[0m init_fn(script_module)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=595'>596</a>\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=596'>597</a>\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_script.py?line=597'>598</a>\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=490'>491</a>\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=491'>492</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=492'>493</a>\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=493'>494</a>\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=495'>496</a>\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=496'>497</a>\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=517'>518</a>\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=519'>520</a>\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=520'>521</a>\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=521'>522</a>\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=522'>523</a>\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py:371\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=367'>368</a>\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m    <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=368'>369</a>\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m--> <a href='file:///home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/jit/_recursive.py?line=370'>371</a>\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n\nforward(__torch__.net.AttnEncoderLayer self, Tensor src, Tensor mask) -> ((Tensor, Tensor)):\nArgument mask not provided.\n:\n  File \"/home/pcy/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n"
     ]
    }
   ],
   "source": [
    "import isaacgym\n",
    "import torch\n",
    "import wandb\n",
    "from net import ActorFixSAC\n",
    "\n",
    "cfg = torch.load('/home/pcy/rl/srl/outputs/2022-06-02/15-23-34/2ho_env.pkl', map_location='cuda:0')\n",
    "save_path = wandb.restore('latest_17.pth', 'jc-bao/corl-2022/23bfj0ac').name\n",
    "with open(save_path, 'rb') as f:\n",
    "  checkpoint = torch.load(f, map_location='cuda:0')\n",
    "act = ActorFixSAC(cfg)\n",
    "act.load_state_dict(checkpoint[\"actor\"], strict=False)\n",
    "scripted_policy = torch.jit.script(act)\n",
    "scripted_policy.save(\"scripted.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f381b2ea382b32a8a0c4ea78237e5104aed4e6c5df67292954c05f7a8793897f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rlgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
